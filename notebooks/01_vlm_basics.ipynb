{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM 基础概念\n",
    "\n",
    "本笔记本介绍 Vision Language Model (VLM) 的基本概念和架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是 VLM？\n",
    "\n",
    "Vision Language Model (VLM) 是一种能够同时理解图像和文本的多模态模型。它能够执行以下任务：\n",
    "\n",
    "- **图像描述** (Image Captioning): 为图像生成文本描述\n",
    "- **视觉问答** (VQA): 根据图像回答问题\n",
    "- **图像检索**: 根据文本搜索相关图像\n",
    "- **文本检索**: 根据图像搜索相关文本\n",
    "- **视觉推理**: 理解图像内容并进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 典型架构\n",
    "\n",
    "大多数 VLM 采用以下架构：\n",
    "\n",
    "```\n",
    "图像 → 图像编码器 → 特征 → 跨模态融合 → 文本解码器 → 文本\n",
    "          (ViT/CNN)    (视觉)    (交叉注意力)    (LLM)\n",
    "```\n",
    "\n",
    "### 2.1 图像编码器\n",
    "- **Vision Transformer (ViT)**: 将图像分割成 patch，用 Transformer 编码\n",
    "- **CNN**: 传统的卷积神经网络如 ResNet\n",
    "\n",
    "### 2.2 文本解码器\n",
    "- **LLM**: 大语言模型如 LLaMA、Bloom 等\n",
    "\n",
    "### 2.3 跨模态融合\n",
    "- **交叉注意力**: 让文本和图像特征相互 attention\n",
    "- **对比学习**: 如 CLIP 使用的对比预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 主要 VLM 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CLIP (Contrastive Language-Image Pre-training)\n",
    "\n",
    "CLIP 是 OpenAI 提出的对比学习模型，包含两个编码器：\n",
    "\n",
    "- **Image Encoder**: ViT 或 ResNet\n",
    "- **Text Encoder**: Transformer\n",
    "\n",
    "训练目标：让匹配的图像-文本对在嵌入空间中更接近\n",
    "\n",
    "```python\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 加载预训练模型\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BLIP (Bootstrapped Language-Image Pre-training)\n",
    "\n",
    "BLIP 是 Salesforce 提出的多模态模型，支持多种任务：\n",
    "\n",
    "- **ITM (Image-Text Matching)**: 判断图像和文本是否匹配\n",
    "- **ITC (Image-Text Contrastive)**: 类似 CLIP 的对比学习\n",
    "- **ENC (Encoder-only)**: 视觉特征编码\n",
    "- **DEC (Decoder-only)**: 图像描述生成\n",
    "\n",
    "```python\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LLaVA (Large Language and Vision Assistant)\n",
    "\n",
    "LLaVA 将 LLM (LLaMA) 与 ViT 结合，通过投影层连接两个模态：\n",
    "\n",
    "```python\n",
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实践：使用 CLIP 进行零样本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 实践代码将在此处添加\n",
    "print(\"VLM 基础学习完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
